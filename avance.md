# ğŸ§¾ Reporte de Avance - Proyecto ViZDoom RL Visual

## ğŸ“… Fecha
Julio 2025

## ğŸ¯ Resumen General

Este documento presenta el avance del proyecto de entrenamiento de un agente de refuerzo profundo (DRL) en el escenario **Defend the Center** del entorno ViZDoom. El agente es entrenado exclusivamente mediante imÃ¡genes procesadas del entorno, sin acceso a variables internas del motor de juego.

---

## ğŸ¤– Modelo Usado: `DQNWithAttention`

Se ha implementado y entrenado un modelo **DQN con Mecanismo de AtenciÃ³n Espacial**, basado en el archivo `dqn_att.py`. Esta arquitectura extiende al modelo clÃ¡sico DQN al incluir una capa de atenciÃ³n que permite al agente concentrarse en regiones especÃ­ficas de la imagen de entrada.

### CaracterÃ­sticas de la arquitectura:

- **Entrada**: Tensores de forma `[4, 100, 160]` (stack de 4 imÃ¡genes en escala de grises).
- **Bloques Convolucionales**: Tres capas Conv2D con `ReLU`, seguidas de normalizaciÃ³n opcional.
- **MÃ³dulo de AtenciÃ³n**:
  - Calcula un mapa de atenciÃ³n sobre las caracterÃ­sticas espaciales.
  - Aplica pesos adaptativos sobre regiones de la imagen.
- **Capa Fully Connected**: DespuÃ©s de la atenciÃ³n, se reduce a un vector de salida de Q-valores para cada acciÃ³n.

Esta atenciÃ³n permite al modelo priorizar Ã¡reas de interÃ©s visual (como enemigos o municiones), mejorando la capacidad de generalizaciÃ³n y velocidad de aprendizaje en entornos visuales complejos.

---

## ğŸ“ˆ Resultados del Entrenamiento

Se ha completado un entrenamiento de **1000 episodios**. El progreso se almacenÃ³ en los checkpoints y se generÃ³ una curva de rendimiento.

### ğŸ–¼ï¸ GrÃ¡fica de Recompensa por episodio

![Curva de entrenamiento](checkpoints/defend_the_center_dqn_a/reward_curve.png)

---

## ğŸ‘ï¸ Procesamiento Visual

El procesamiento visual es fundamental para el aprendizaje del agente, ya que las decisiones se basan exclusivamente en lo que "ve". Para ello se ha creado una clase central: `GrayscaleProcessor`.

### Funcionalidades implementadas:

- **ConversiÃ³n a escala de grises**: reduce la dimensionalidad y resalta formas.
- **Redimensionamiento uniforme**: se reescalan las imÃ¡genes a `[100 x 160]`, normalizando el input para la red.
- **NormalizaciÃ³n**: valores de pÃ­xel llevados a rango `[0, 1]`.
- **DetecciÃ³n de bordes**: realce de contornos y siluetas mediante kernels.

### Posibilidades adicionales ya contempladas:

- AplicaciÃ³n de filtros como:
  - Sharpening (agudizado)
  - Gaussian Blur
  - Thresholding adaptativo

Esto permite una alta flexibilidad en la experimentaciÃ³n con distintas representaciones visuales para evaluar su impacto en el rendimiento del agente.

---

## ğŸ§ª Experimentos en Curso

- Pruebas comparativas con modelo `DQN` base (sin atenciÃ³n).
- EvaluaciÃ³n del impacto de los filtros visuales en el aprendizaje.

---

## ğŸ“Œ Conclusiones Parciales

- El uso de atenciÃ³n espacial ha mostrado una mejora temprana en la convergencia del agente.
- El procesamiento visual robusto ha permitido entrenar sin acceso a variables internas del juego.
- El agente aprende Ãºnicamente a travÃ©s de imÃ¡genes, acercÃ¡ndose a un control visual autÃ³nomo.

---


