# üßæ Reporte de Avance - Proyecto ViZDoom RL Visual

## üìÖ Fecha
Julio 2025

## üéØ Resumen General

Este documento presenta el avance del proyecto de entrenamiento de un agente de refuerzo profundo (DRL) en el escenario **Defend the Center** del entorno ViZDoom. El agente es entrenado exclusivamente mediante im√°genes procesadas del entorno, sin acceso a variables internas del motor de juego.

---

## ü§ñ Modelo Usado: `DQNWithAttention`

Se ha implementado y entrenado un modelo **DQN con Mecanismo de Atenci√≥n Espacial**, basado en el archivo `dqn_att.py`. Esta arquitectura extiende al modelo cl√°sico DQN al incluir una capa de atenci√≥n que permite al agente concentrarse en regiones espec√≠ficas de la imagen de entrada.

### Caracter√≠sticas de la arquitectura:

- **Entrada**: Tensores de forma `[4, 100, 160]` (stack de 4 im√°genes en escala de grises).
- **Bloques Convolucionales**: Tres capas Conv2D con `ReLU`, seguidas de normalizaci√≥n opcional.
- **M√≥dulo de Atenci√≥n**:
  - Calcula un mapa de atenci√≥n sobre las caracter√≠sticas espaciales.
  - Aplica pesos adaptativos sobre regiones de la imagen.
- **Capa Fully Connected**: Despu√©s de la atenci√≥n, se reduce a un vector de salida de Q-valores para cada acci√≥n.

Esta atenci√≥n permite al modelo priorizar √°reas de inter√©s visual (como enemigos o municiones), mejorando la capacidad de generalizaci√≥n y velocidad de aprendizaje en entornos visuales complejos.

---

## üìà Resultados del Entrenamiento

Se ha completado un entrenamiento de **1000 episodios**. El progreso se almacen√≥ en los checkpoints y se gener√≥ una curva de rendimiento.

### üñºÔ∏è Gr√°fica de Recompensa Promedio

![Curva de entrenamiento](checkpoints/curva_entrenamiento.png)

> *La curva muestra un incremento sostenido en la recompensa promedio por episodio, lo cual indica que el agente ha aprendido una pol√≠tica efectiva de supervivencia y ataque en el escenario.*

---

## üëÅÔ∏è Procesamiento Visual

El procesamiento visual es fundamental para el aprendizaje del agente, ya que las decisiones se basan exclusivamente en lo que "ve". Para ello se ha creado una clase central: `GrayscaleProcessor`.

### Funcionalidades implementadas:

- **Conversi√≥n a escala de grises**: reduce la dimensionalidad y resalta formas.
- **Redimensionamiento uniforme**: se reescalan las im√°genes a `[100 x 160]`, normalizando el input para la red.
- **Normalizaci√≥n**: valores de p√≠xel llevados a rango `[0, 1]`.
- **Apilamiento de frames**: cuatro frames consecutivos se agrupan como un solo input (captura del movimiento).
- **Detecci√≥n de bordes** (opcional): realce de contornos y siluetas mediante kernels.
- **Mapas de calor o etiquetas**: integraci√≥n futura con detecci√≥n de enemigos mediante `labels_buffer`.

### Posibilidades adicionales ya contempladas:

- Canal auxiliar con detecci√≥n de enemigos (YOLO).
- Mapa sem√°ntico o m√°scara binaria para entidades clave.
- Aplicaci√≥n de filtros como:
  - Sharpening (agudizado)
  - Gaussian Blur
  - Thresholding adaptativo

Esto permite una alta flexibilidad en la experimentaci√≥n con distintas representaciones visuales para evaluar su impacto en el rendimiento del agente.

---

## üß™ Experimentos en Curso

- Pruebas comparativas con modelo `DQN` base (sin atenci√≥n).
- Evaluaci√≥n del impacto de los filtros visuales en el aprendizaje.
- Medici√≥n del tiempo de entrenamiento vs rendimiento del agente.

---

## üìå Conclusiones Parciales

- El uso de atenci√≥n espacial ha mostrado una mejora temprana en la convergencia del agente.
- El procesamiento visual robusto ha permitido entrenar sin acceso a variables internas del juego.
- El agente aprende √∫nicamente a trav√©s de im√°genes, acerc√°ndose a un control visual aut√≥nomo.

---

## üîú Pr√≥ximos Pasos

- Realizar entrenamiento extendido a 5000 episodios.
- Integrar canales adicionales con informaci√≥n sem√°ntica.
- Evaluar modelos con y sin atenci√≥n bajo las mismas condiciones.
- Documentar resultados y preparar publicaci√≥n t√©cnica.

