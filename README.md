# ViZDoom RL Project ğŸ§ ğŸ®

Este proyecto entrena agentes de aprendizaje por refuerzo profundo (DQN y variantes) en entornos del juego [ViZDoom](https://vizdoom.cs.put.edu.pl/) utilizando distintos tipos de entradas visuales y variables internas del juego. El objetivo es evaluar cÃ³mo distintas representaciones de la observaciÃ³n (procesamiento de imÃ¡genes y entradas adicionales) afectan el rendimiento del agente.

---

## ğŸš€ Objetivo del Proyecto

Entrenar agentes inteligentes que puedan aprender a jugar escenarios de ViZDoom mediante tÃ©cnicas de Aprendizaje por Refuerzo Profundo. Se evalÃºan tres configuraciones diferentes para las observaciones que recibe el agente:

1. **Canny + Escala de Grises**
   (`config_defend_the_center.yaml`)
2. **Canny + Escala de Grises + Mapa de Profundidad**
   (`config_defend_the_center_profundidad.yaml`)
3. **(2) + Variables del juego** (como vida y municiÃ³n) y arquitectura recurrente
   (`config_defend_the_center_recurrente.yaml`)

---

## ğŸ§© Estructura del Repositorio

```
vizdoom_rl_project/
â”œâ”€â”€ config/                              # Configuraciones YAML de entrenamiento
â”‚   â”œâ”€â”€ config_defend_the_center.yaml
â”‚   â”œâ”€â”€ config_defend_the_center_profundidad.yaml
â”‚   â””â”€â”€ config_defend_the_center_recurrente.yaml
â”œâ”€â”€ env/                                 # DefiniciÃ³n de entorno personalizado (wrapper Gym)
â”œâ”€â”€ models/                              # Modelos de red neuronal
â”œâ”€â”€ checkpoints/                         # Pesos guardados de modelos entrenados
â”œâ”€â”€ utils/                               # Herramientas auxiliares: memoria de experiencia, visualizaciones
â”œâ”€â”€ main.py                              # Script principal de entrenamiento
â”œâ”€â”€ eval.py                              # EvaluaciÃ³n de agentes entrenados
â”œâ”€â”€ requirements.txt                     # Dependencias del proyecto
â”œâ”€â”€ propuesta.md                         # Documento con idea inicial del proyecto
â””â”€â”€ README.md                            # Este archivo
```

---

## ğŸ› ï¸ Requisitos

Este proyecto ha sido probado con:

* **Python 3.9**
* **ViZDoom**
* **PyTorch**
* **OpenCV**
* **NumPy**
* **Matplotlib**
* **tqdm**

Instala todas las dependencias con:

```bash
pip install -r requirements.txt
```

---

## âš™ï¸ InstalaciÃ³n

1. Clona el repositorio:

```bash
git clone https://github.com/smindiol/vizdoom_rl_project.git
cd vizdoom_rl_project
```

2. Crea y activa un entorno virtual (opcional pero recomendado):

```bash
python -m venv pviz
# En Linux/macOS:
source pviz/bin/activate
# En Windows:
pviz\Scripts\activate.bat
```

3. Instala las dependencias:

```bash
pip install -r requirements.txt
```

---

## ğŸ§ª CÃ³mo Probar el Proyecto

Una vez instalado, puedes ejecutar una evaluaciÃ³n rÃ¡pida con un modelo ya entrenado (por defecto se usa el de profundidad):

```bash
python eval.py --config config/config_defend_the_center_profundidad.yaml --model checkpoints/modelo_entrenado.pth
```

---

## ğŸ‹ï¸â€â™‚ï¸ CÃ³mo Entrenar un Agente

Puedes entrenar un agente con:

```bash
python main.py --config config/config_defend_the_center.yaml
```

Otras configuraciones disponibles:

* `config/config_defend_the_center.yaml`: utiliza imÃ¡genes en escala de grises + bordes Canny.
* `config/config_defend_the_center_profundidad.yaml`: aÃ±ade mapa de profundidad.
* `config/config_defend_the_center_recurrente.yaml`: aÃ±ade variables internas del juego (vida, municiÃ³n) y usa red recurrente.

Ejemplo:

```bash
python main.py --config config/config_defend_the_center_profundidad.yaml
python main.py --config config/config_defend_the_center_recurrente.yaml
```

Los modelos se guardarÃ¡n automÃ¡ticamente en la carpeta `checkpoints/`.

---

## ğŸ§  Detalles TÃ©cnicos

* El entorno estÃ¡ basado en Gym, usando una clase personalizada que envuelve ViZDoom.
* Las imÃ¡genes de entrada son procesadas con OpenCV para extraer bordes (Canny) y/o mapas de profundidad.
* Se usa un frame-skip para acelerar el entrenamiento.
* Algunas arquitecturas usan mecanismos de atenciÃ³n espacial y redes recurrentes para mejorar la toma de decisiones.
* Las variables del juego (vida, municiÃ³n) se integran como entradas adicionales a la red.

---

## ğŸ“Š EvaluaciÃ³n

El rendimiento del agente se evalÃºa promediando la recompensa total por episodio en distintos escenarios, comparando los resultados obtenidos con distintas entradas (observaciones simples vs enriquecidas).

---


## ğŸ‘¨â€ğŸ’» Autor

Desarrollado por [Sebastian Mindiola](https://github.com/smindiol)
Repositorio: [https://github.com/smindiol/vizdoom\_rl\_project](https://github.com/smindiol/vizdoom_rl_project)
